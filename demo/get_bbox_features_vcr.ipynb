{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your runtime prior to this, to let your installation take effect\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "import torch\n",
    "\n",
    "from detectron2.modeling.postprocessing import detector_postprocess\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference_single_image\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.structures.instances import Instances\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doit(raw_image, raw_boxes):\n",
    "        # Process Boxes\n",
    "    raw_boxes = Boxes(torch.from_numpy(raw_boxes).cuda())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        raw_height, raw_width = raw_image.shape[:2]\n",
    "        \n",
    "        # Preprocessing\n",
    "        image = predictor.transform_gen.get_transform(raw_image).apply_image(raw_image)\n",
    "        \n",
    "        # Scale the box\n",
    "        new_height, new_width = image.shape[:2]\n",
    "        scale_x = 1. * new_width / raw_width\n",
    "        scale_y = 1. * new_height / raw_height\n",
    "        #print(scale_x, scale_y)\n",
    "        boxes = raw_boxes.clone()\n",
    "        boxes.scale(scale_x=scale_x, scale_y=scale_y)\n",
    "        \n",
    "        # ----\n",
    "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "        inputs = [{\"image\": image, \"height\": raw_height, \"width\": raw_width}]\n",
    "        images = predictor.model.preprocess_image(inputs)\n",
    "        \n",
    "        # Run Backbone Res1-Res4\n",
    "        features = predictor.model.backbone(images.tensor)\n",
    "        \n",
    "        # Run RoI head for each proposal (RoI Pooling + Res5)\n",
    "        proposal_boxes = [boxes]\n",
    "        features = [features[f] for f in predictor.model.roi_heads.in_features]\n",
    "        box_features = predictor.model.roi_heads._shared_roi_transform(\n",
    "            features, proposal_boxes\n",
    "        )\n",
    "        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n",
    "        return feature_pooled       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_key = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_dataset_id_to_contiguous_id\n",
    "coco_key = {coco_key[k]: k for k in coco_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = \"/home/jamesp/data/vcr/vcr1images\"\n",
    "movie_dirs = sorted(os.listdir(IMAGE_DIR))\n",
    "print(len(movie_dirs))\n",
    "OUTPUT_DIR = \"/home/jamesp/data/visualcomet/features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "for movie in tqdm(movie_dirs[-2:]):\n",
    "    img_ids = list(set([id[:id.rfind('.')] for id in os.listdir(os.path.join(IMAGE_DIR,movie))]))\n",
    "    for id in sorted(img_ids):\n",
    "        im = cv2.imread(os.path.join(IMAGE_DIR,movie,id+'.jpg'))\n",
    "        metadata = json.load(open(os.path.join(IMAGE_DIR,movie,id+'.json')))\n",
    "        boxes = np.array(metadata['boxes'])[:,:4]\n",
    "        h = metadata['height']\n",
    "        w = metadata['width']\n",
    "        boxes = np.row_stack((np.array([0,0,w,h]),boxes))\n",
    "        obj_rep = doit(im, boxes).to(\"cpu\").numpy()\n",
    "        \n",
    "        features = {'image_features' : obj_rep[0],\n",
    "                    'object_features' : obj_rep[1:]}\n",
    "        pickle.dump(features, open(os.path.join(OUTPUT_DIR,id+'.pkl'),'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
